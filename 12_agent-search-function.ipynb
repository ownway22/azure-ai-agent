{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3d4685",
   "metadata": {},
   "source": [
    "# AI Search02 - Agentic retrieval using Azure AI Search and Azure AI Agent Service\n",
    "\n",
    "Use this notebook to create an agentic retrieval pipeline built on Azure AI Search and an Azure AI Agent.\n",
    "\n",
    "In this walkthrough, you will:\n",
    "\n",
    "+ Create an \"earth_at_night\" search index\n",
    "+ Load it with documents from a GitHub URL\n",
    "+ Create a knowledge agent on Azure AI Search that points to an LLM for intelligent query planning\n",
    "+ Create a Foundry agent in Azure AI Foundry to determine when queries are needed\n",
    "+ Create a Azure AI Agent tool (client) to orchestrate all requests\n",
    "+ Start a chat with the agent\n",
    "\n",
    "This notebook is referenced in [Build an agentic retrieval pipeline in Azure AI Search](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-pipeline).\n",
    "\n",
    "This exercise differs from the [Agentic Retrieval Quickstart](https://learn.microsoft.com/azure/search/search-get-started-agentic-retrieval) in how it uses Azure AI Agent to determine whether to retrieve data from the index, and how it uses an agent tool for orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd68a6e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "+ Azure AI Search, basic tier or higher, in any region that supports semantic ranker.\n",
    "\n",
    "+ Azure OpenAI, and you should have an **Azure AI Developer** role assignment to create a Foundry project.\n",
    "\n",
    "+ An [Azure AI agent and Foundry project](https://learn.microsoft.com/azure/ai-services/agents/quickstart?pivots=ai-foundry-portal), created in the Azure AI Foundry portal, with the basic setup, used for creating the Foundry agent.\n",
    "\n",
    "+ A deployment of a [supported model](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-create#supported-models) in your Foundry project. This notebook uses gpt-4o-mini. We recommend 100,000 token capacity. You can find capacity and the rate limit in the model deployments list in the Azure AI Foundry portal.\n",
    "\n",
    "We recommend creating a virtual environment to run this sample code. In Visual Studio Code, open the control palette (ctrl-shift-p) to create an environment. This notebook was tested on Python 3.10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40a871",
   "metadata": {},
   "source": [
    "## Set up connections\n",
    "\n",
    " `.env` modify the environment variables to use your Azure endpoints. You need endpoints for:\n",
    "\n",
    "+ Azure AI Search\n",
    "+ Azure OpenAI\n",
    "+ Azure AI Foundry project\n",
    "\n",
    "You can find endpoints for Azure AI Search and Azure OpenAI in the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "You can find the project endpoint in the [Azure AI Foundry portal](https://ai.azure.com).\n",
    "   A hypothetical endpoint might look like this: `https://your-foundry-resource.services.ai.azure.com/api/projects/your-foundry-project`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d766f",
   "metadata": {},
   "source": [
    "[Checkpoint 3]\n",
    "![alt text](Image\\image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bc80a",
   "metadata": {},
   "source": [
    "## Load Connections\n",
    "\n",
    "Load the environment variables to set up connections and object names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e42b4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file (override=True ensures .env values take precedence)\n",
    "load_dotenv(override=True) \n",
    "\n",
    "# Define default deployment names for Azure OpenAI and Azure Search index\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT=\"gpt-4.1-mini\"\n",
    "AZURE_SEARCH_INDEX_NAME=\"12-earth-at-night\"\n",
    "project_endpoint = os.environ[\"PROJECT_ENDPOINT\"]\n",
    "agent_model = os.getenv(\"AGENT_MODEL\", \"gpt-4.1-mini\")\n",
    "endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "\n",
    "# Authentication setup using Azure's default credential chain\n",
    "# This will try managed identity, Azure CLI, Visual Studio, etc. in order\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://search.azure.com/.default\")\n",
    "\n",
    "# Search index name - note the default differs from the constant above\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"12-earth-at-night\")\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]  # Your Azure OpenAI service endpoint\n",
    "azure_openai_gpt_deployment = os.getenv(\"AZURE_OPENAI_GPT_DEPLOYMENT\", \"gpt-4.1-mini\")\n",
    "azure_openai_gpt_model = os.getenv(\"AZURE_OPENAI_GPT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "# Embedding model deployment settings for vector search\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "\n",
    "# Agent name for both Azure Search knowledge agent and AI Foundry agent\n",
    "agent_name = os.getenv(\"AZURE_SEARCH_AGENT_NAME\", \"12-earth-search-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ecdce",
   "metadata": {},
   "source": [
    "## Create search index on Azure AI Search\n",
    "\n",
    "This steps create a search index that contains plain text and vector content. You can use any existing search index, but it must meet the [criteria for agentic retrieval workloads](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-index). The primary schmea requirement is that is has a semantic configuration, with a `default_configuration_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91fd6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index '12-earth-at-night' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndex, SearchField, VectorSearch, VectorSearchProfile, HnswAlgorithmConfiguration, AzureOpenAIVectorizer, AzureOpenAIVectorizerParameters, SemanticSearch, SemanticConfiguration, SemanticPrioritizedFields, SemanticField\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# Create a comprehensive search index that supports both text and vector search\n",
    "# This index is designed for agentic retrieval workloads with semantic capabilities\n",
    "index = SearchIndex(\n",
    "    name=index_name,  # Use the index name from environment configuration\n",
    "    fields=[\n",
    "        # Primary key field - required for all Azure Search indexes\n",
    "        SearchField(name=\"id\", type=\"Edm.String\", key=True, filterable=True, sortable=True, facetable=True),\n",
    "        \n",
    "        # Text content field containing the actual document chunks\n",
    "        # This field is searchable but not filterable/sortable to optimize for text search\n",
    "        SearchField(name=\"page_chunk\", type=\"Edm.String\", filterable=False, sortable=False, facetable=False),\n",
    "        \n",
    "        # Vector embedding field for semantic similarity search\n",
    "        # - Collection(Edm.Single): Array of floating-point numbers representing the embedding\n",
    "        # - stored=False: Don't store the raw embeddings (saves space, still searchable)\n",
    "        # - vector_search_dimensions=3072: Dimension size for text-embedding-3-large model\n",
    "        # - vector_search_profile_name: Links to the vector search configuration below\n",
    "        SearchField(name=\"page_embedding_text_3_large\", type=\"Collection(Edm.Single)\", stored=False, vector_search_dimensions=3072, vector_search_profile_name=\"hnsw_text_3_large\"),\n",
    "        \n",
    "        # Page number field for filtering and sorting results by document location\n",
    "        SearchField(name=\"page_number\", type=\"Edm.Int32\", filterable=True, sortable=True, facetable=True)\n",
    "    ],\n",
    "    \n",
    "    # Vector search configuration using HNSW (Hierarchical Navigable Small World) algorithm\n",
    "    vector_search=VectorSearch(\n",
    "        # Vector search profiles define how vector fields are searched\n",
    "        profiles=[VectorSearchProfile(\n",
    "            name=\"hnsw_text_3_large\",  # Profile name referenced by vector fields\n",
    "            algorithm_configuration_name=\"alg\",  # Links to algorithm configuration\n",
    "            vectorizer_name=\"azure_openai_text_3_large\"  # Links to vectorizer configuration\n",
    "        )],\n",
    "        \n",
    "        # HNSW algorithm configuration for approximate nearest neighbor search\n",
    "        # HNSW provides good balance between search speed and accuracy\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"alg\")],\n",
    "        \n",
    "        # Vectorizer configuration - defines how text is converted to embeddings\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"azure_openai_text_3_large\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=azure_openai_endpoint,  # Your Azure OpenAI endpoint\n",
    "                    deployment_name=azure_openai_embedding_deployment,  # Embedding model deployment\n",
    "                    model_name=azure_openai_embedding_model  # Embedding model name\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    # Semantic search configuration for intelligent ranking and understanding\n",
    "    # This is required for agentic retrieval workloads\n",
    "    semantic_search=SemanticSearch(\n",
    "        default_configuration_name=\"semantic_config\",  # Default semantic configuration\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"semantic_config\",\n",
    "                # Define which fields are prioritized for semantic understanding\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    # Content fields contain the main text content for semantic analysis\n",
    "                    content_fields=[\n",
    "                        SemanticField(field_name=\"page_chunk\")  # Use our text content field\n",
    "                    ]\n",
    "                    # Note: We could also define title_fields and keyword_fields for richer semantic understanding\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the search index client and deploy the index\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "# Create or update the index (idempotent operation)\n",
    "# If index exists, it will be updated with new schema; if not, it will be created\n",
    "index_client.create_or_update_index(index)\n",
    "print(f\"Index '{index_name}' created or updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927dad23",
   "metadata": {},
   "source": [
    "[Checkpoint 4]\n",
    "![alt text](Image\\image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b9785",
   "metadata": {},
   "source": [
    "## Upload sample documents\n",
    "\n",
    "This sample uses data from NASA's Earth at Night e-book. It's retrieved from the sample data GitHub repository and passed to the search client for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f98f31e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents uploaded to index '12-earth-at-night'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "# URL pointing to pre-processed NASA Earth at Night e-book data\n",
    "# This data is already chunked and formatted for search indexing\n",
    "url = \"https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json\"\n",
    "\n",
    "# Download the sample documents from GitHub\n",
    "# The documents are already in the correct JSON format with fields matching our index schema\n",
    "documents = requests.get(url).json()\n",
    "\n",
    "# Use SearchIndexingBufferedSender for efficient batch uploading\n",
    "# This class automatically handles batching, retries, and error handling\n",
    "# The 'with' statement ensures proper resource cleanup\n",
    "with SearchIndexingBufferedSender(\n",
    "    endpoint=endpoint,           # Azure Search service endpoint\n",
    "    index_name=index_name,      # Target index name\n",
    "    credential=credential       # Authentication credentials\n",
    ") as client:\n",
    "    # Upload all documents in batches\n",
    "    # The sender will automatically:\n",
    "    # - Split documents into optimal batch sizes\n",
    "    # - Generate embeddings using the configured vectorizer\n",
    "    # - Handle retries for failed uploads\n",
    "    # - Provide status updates\n",
    "    client.upload_documents(documents=documents)\n",
    "\n",
    "print(f\"Documents uploaded to index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0081e",
   "metadata": {},
   "source": [
    "## Create a knowledge agent on Azure AI Search\n",
    "\n",
    "This steps creates a knowledge agent on Azure AI Search. This agent is a wrapper to a large language model, used for sending queries to an agentic retrieval pipeline. The maximum output size refers to the query response. Setting this value helps you control token usage and how many tokens are sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbe31e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge agent '12-earth-search-agent' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "# Import classes for creating and managing Azure Search knowledge agents\n",
    "from azure.search.documents.indexes.models import KnowledgeAgent, KnowledgeAgentAzureOpenAIModel, KnowledgeAgentTargetIndex, KnowledgeAgentRequestLimits, AzureOpenAIVectorizerParameters\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# Create a knowledge agent - an AI-powered search orchestrator\n",
    "# The knowledge agent acts as an intelligent query planner that:\n",
    "# - Analyzes user queries for intent and complexity\n",
    "# - Decides what search strategies to use (keyword, vector, semantic, hybrid)\n",
    "# - Plans multi-step retrieval when needed\n",
    "# - Formats and ranks results for optimal relevance\n",
    "agent = KnowledgeAgent(\n",
    "    name=agent_name,  # Unique name for this knowledge agent\n",
    "    \n",
    "    # Configure the LLM models that power the agent's intelligence\n",
    "    models=[\n",
    "        KnowledgeAgentAzureOpenAIModel(\n",
    "            # Connect to your Azure OpenAI GPT deployment\n",
    "            azure_open_ai_parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,        # Your Azure OpenAI service endpoint\n",
    "                deployment_name=azure_openai_gpt_deployment, # GPT model deployment name\n",
    "                model_name=azure_openai_gpt_model           # GPT model name (e.g., gpt-4.1-mini)\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    # Define which search indexes this agent can query\n",
    "    target_indexes=[\n",
    "        KnowledgeAgentTargetIndex(\n",
    "            index_name=index_name,          # The search index we created earlier\n",
    "            default_reranker_threshold=2.5  # Minimum relevance score for including results\n",
    "                                           # Higher values = more selective results\n",
    "                                           # Lower values = more comprehensive results\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    # Set limits to control resource usage and response quality\n",
    "    request_limits=KnowledgeAgentRequestLimits(\n",
    "        max_output_size=10000  # Maximum number of tokens in agent responses\n",
    "                              # Helps control costs and ensures responses fit in context windows\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the search index client and deploy the knowledge agent\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "# Create or update the knowledge agent (idempotent operation)\n",
    "# The agent becomes available immediately after creation\n",
    "index_client.create_or_update_agent(agent)\n",
    "print(f\"Knowledge agent '{agent_name}' created or updated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "711fb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge agent '12-earth-search-agent' is available and working.\n"
     ]
    }
   ],
   "source": [
    "# Verify that the knowledge agent was created successfully and is accessible\n",
    "# This is important because agent creation might succeed but the agent might not be immediately available\n",
    "try:\n",
    "    # Retrieve agent information from Azure Search\n",
    "    agent_info = index_client.get_agent(agent_name)\n",
    "    print(f\"Knowledge agent '{agent_info.name}' is available and working.\")\n",
    "except Exception as e:\n",
    "    print(f\"Knowledge agent '{agent_name}' is not available or not working: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff845de0",
   "metadata": {},
   "source": [
    "## Create an Azure AI Agent\n",
    "\n",
    "In the Azure AI Foundry, an agent is a smart micro-service that can do RAG. The purpose of this specific agent is to decide when to send a query to the agentic retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c21c70df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'asst_a6cWPZOs27r61GKiWi671INY', 'object': 'assistant', 'created_at': 1753014478, 'name': '13-earth-search-agent', 'description': None, 'model': 'gpt-4.1-mini', 'instructions': '\\nA Q&A agent that can answer questions about the Earth at night.\\nSources have a JSON format with a ref_id that must be cited in the answer using the format [ref_id].\\nIf you do not have the answer, respond with \"I don\\'t know\".\\n', 'tools': [], 'top_p': 1.0, 'temperature': 1.0, 'tool_resources': {}, 'metadata': {}, 'response_format': 'auto'},\n",
       " {'id': 'asst_hHwJMSNzJLg0YggUmhEZW744', 'object': 'assistant', 'created_at': 1753013020, 'name': '12-earth-search-agent', 'description': None, 'model': 'gpt-4.1-mini', 'instructions': '\\nA Q&A agent that can answer questions about the Earth at night.\\nSources have a JSON format with a ref_id that must be cited in the answer using the format [ref_id].\\nIf you do not have the answer, respond with \"I don\\'t know\".\\n', 'tools': [], 'top_p': 1.0, 'temperature': 1.0, 'tool_resources': {}, 'metadata': {}, 'response_format': 'auto'},\n",
       " {'id': 'asst_8YdaB2D491hheiNSs2AQHWeN', 'object': 'assistant', 'created_at': 1752993460, 'name': 'Agent_General', 'description': None, 'model': 'gpt-4o', 'instructions': 'you are an expert of finance, ', 'tools': [], 'top_p': 1.0, 'temperature': 1.0, 'tool_resources': {}, 'metadata': {}, 'response_format': 'auto'},\n",
       " {'id': 'asst_Hx5NFJb07ld153t0IdzYaTn9', 'object': 'assistant', 'created_at': 1752990202, 'name': 'Agent_trendmicro', 'description': None, 'model': 'gpt-4o', 'instructions': 'you are an expert of finance, you will answer the information base on the knowledge, if not came from the knowledge, you should start the reply by \"AI :\"', 'tools': [{'type': 'azure_ai_search'}], 'top_p': 1.0, 'temperature': 1.0, 'tool_resources': {'azure_ai_search': {'indexes': [{'index_connection_id': None, 'index_name': None, 'query_type': 'simple', 'top_k': 9, 'filter': None, 'index_asset_id': 'trendmicro01/versions/1'}]}}, 'metadata': {}, 'response_format': 'auto'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Azure AI Projects client for managing AI agents and workflows\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Create a client for the Azure AI Foundry project\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,  # Your AI Foundry project endpoint\n",
    "    credential=credential       # Use the same Azure credential for authentication\n",
    ")\n",
    "\n",
    "# List all existing agents in the project (for debugging/verification)\n",
    "list(project_client.agents.list_agents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d55eda9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI agent '12-earth-search-agent' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "# Define the system instructions for the AI agent\n",
    "instructions = \"\"\"\n",
    "A Q&A agent that can answer questions about the Earth at night.\n",
    "Sources have a JSON format with a ref_id that must be cited in the answer using the format [ref_id].\n",
    "If you do not have the answer, respond with \"I don't know\".\n",
    "\"\"\"\n",
    "# Key aspects of these instructions:\n",
    "# - Domain-specific: Focused on Earth at night topics\n",
    "# - Citation requirement: Must reference sources with [ref_id] format\n",
    "# - Honest about limitations: Says \"I don't know\" when information isn't available\n",
    "# - Structured responses: Expects JSON-formatted source data\n",
    "\n",
    "# Create the AI agent in Azure AI Foundry\n",
    "# This agent will orchestrate the entire RAG (Retrieval-Augmented Generation) pipeline\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=agent_model,      # The LLM model to use (e.g., gpt-4.1-mini)\n",
    "    name=agent_name,        # Agent name (same as knowledge agent for consistency)\n",
    "    instructions=instructions  # System prompt that defines agent behavior\n",
    ")\n",
    "# The agent will:\n",
    "# - Decide when to retrieve information from the search index\n",
    "# - Process user queries intelligently\n",
    "# - Coordinate with tools (like our agentic retrieval function)\n",
    "# - Format responses according to the instructions\n",
    "\n",
    "print(f\"AI agent '{agent_name}' created or updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc4098",
   "metadata": {},
   "source": [
    "[Checkpoint 5]\n",
    "![alt text](Image\\image5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f99d1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI agent '12-earth-search-agent' is available and working.\n"
     ]
    }
   ],
   "source": [
    "# Verify that the AI agent was created successfully in the Foundry project\n",
    "# This ensures the agent is ready to handle conversations and tool calls\n",
    "try:\n",
    "    # Retrieve agent information using the agent ID returned from creation\n",
    "    agent_info = project_client.agents.get_agent(agent.id)\n",
    "    print(f\"AI agent '{agent_info.name}' is available and working.\")\n",
    "except Exception as e:\n",
    "    print(f\"AI agent '{agent_name}' is not available or not working: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a051e",
   "metadata": {},
   "source": [
    "## Add an agentic retrieval tool to AI Agent\n",
    "\n",
    "An end-to-end pipeline needs an orchestration mechanism for coordinating calls to the retriever and agent. The pattern described in this notebook uses a [tool](https://learn.microsoft.com/azure/ai-services/agents/how-to/tools/function-calling) for this task. The tool calls the Azure AI Search knowledge retrieval client and the Azure AI agent, and it drives the conversations with the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de2ee775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.agents.models import FunctionTool, ToolSet, ListSortOrder\n",
    "from azure.search.documents.agent import KnowledgeAgentRetrievalClient\n",
    "from azure.search.documents.agent.models import KnowledgeAgentRetrievalRequest, KnowledgeAgentMessage, KnowledgeAgentMessageTextContent, KnowledgeAgentIndexParams\n",
    "\n",
    "# Create a knowledge retrieval client that connects to our Azure Search knowledge agent\n",
    "# This client handles the communication between the AI agent and the search index\n",
    "agent_client = KnowledgeAgentRetrievalClient(\n",
    "    endpoint=endpoint,          # Azure Search service endpoint\n",
    "    agent_name=agent_name,      # Name of the knowledge agent we created\n",
    "    credential=credential       # Authentication credentials\n",
    ")\n",
    "\n",
    "# Create a conversation thread for managing multi-turn conversations\n",
    "# Threads maintain context across multiple exchanges with the agent\n",
    "thread = project_client.agents.threads.create()\n",
    "\n",
    "# Dictionary to store retrieval results mapped to message IDs\n",
    "# This allows us to access detailed retrieval information later for analysis\n",
    "retrieval_results = {}\n",
    "\n",
    "def agentic_retrieval() -> str:\n",
    "    \"\"\"\n",
    "    Agentic retrieval function that searches NASA e-book data about Earth at night.\n",
    "    \n",
    "    This function serves as a \"tool\" that the AI agent can call when it needs to\n",
    "    retrieve information from the search index. The agent decides autonomously\n",
    "    when to use this tool based on the user's query.\n",
    "    \n",
    "    Returns:\n",
    "        str: JSON-formatted search results with reference IDs for citation\n",
    "    \n",
    "    The function performs these steps:\n",
    "    1. Gets recent conversation history for context\n",
    "    2. Sends the conversation to the knowledge agent for intelligent retrieval\n",
    "    3. Stores detailed retrieval results for later analysis\n",
    "    4. Returns the formatted response to the AI agent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the last 5 messages from the conversation thread\n",
    "    # This provides context for the knowledge agent to understand the query intent\n",
    "    # DESCENDING order gets the most recent messages first\n",
    "    messages = project_client.agents.messages.list(\n",
    "        thread.id, \n",
    "        limit=5, \n",
    "        order=ListSortOrder.DESCENDING\n",
    "    )\n",
    "    \n",
    "    # Convert iterator to list and reverse to get chronological order\n",
    "    # (oldest message first, newest message last)\n",
    "    # This helps the knowledge agent understand conversation flow\n",
    "    messages = list(messages)\n",
    "    messages.reverse()\n",
    "    \n",
    "    # Send the conversation context to the knowledge agent for intelligent retrieval\n",
    "    retrieval_result = agent_client.retrieve(\n",
    "        retrieval_request=KnowledgeAgentRetrievalRequest(\n",
    "            # Convert messages to the format expected by the knowledge agent\n",
    "            # Filter out system messages as they don't contain user queries\n",
    "            messages=[\n",
    "                KnowledgeAgentMessage(\n",
    "                    role=msg[\"role\"], \n",
    "                    content=[KnowledgeAgentMessageTextContent(text=msg.content[0].text)]\n",
    "                ) \n",
    "                for msg in messages \n",
    "                if msg[\"role\"] != \"system\"\n",
    "            ],\n",
    "            \n",
    "            # Configure the target index and search parameters\n",
    "            target_index_params=[\n",
    "                KnowledgeAgentIndexParams(\n",
    "                    index_name=index_name,      # Search in our Earth at night index\n",
    "                    reranker_threshold=2.5      # Minimum relevance score for results\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Store the detailed retrieval results for later analysis\n",
    "    # Associate with the most recent message in the conversation\n",
    "    last_message = messages[-1]\n",
    "    retrieval_results[last_message.id] = retrieval_result\n",
    "    \n",
    "    # Return the grounding response to the AI agent\n",
    "    # This contains the search results formatted for the agent to use in its response\n",
    "    return retrieval_result.response[0].content[0].text\n",
    "\n",
    "# Create a function tool from our agentic retrieval function\n",
    "# The AI agent can call this tool when it determines that information retrieval is needed\n",
    "# Reference: https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/function-calling\n",
    "functions = FunctionTool({ agentic_retrieval })\n",
    "\n",
    "# Create a toolset and add our function tool\n",
    "# Toolsets group related tools that an agent can use\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "# Enable automatic function calling for the AI agent\n",
    "# This allows the agent to automatically decide when to call our agentic_retrieval function\n",
    "# The agent will analyze user queries and determine if search is needed\n",
    "project_client.agents.enable_auto_function_calls(toolset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "babad0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent target indexes: ['12-earth-at-night']\n",
      "Retrieval response: [{\"ref_id\":0,\"content\":\"interactions between society and natural processes. Beyond academic study, Earth nightlight measurements are being used to help save lives and property around the globe, by allowing accurate identification and monitoring of ongoing events like eruptions and fires even in remote locations, and by pinpointing and enabling quantitative tracking of regions of power outage and recovery following extreme weather events and geohazards in populated areas.\\n\\nEarth at Night tells—in the words of the women and men, the scientists and engineers who are actually designing the instruments and conducting the analyses of Earth's nocturnal illumination imagery—the story of satellite measurements of global light in the night. It shows how ever-increasing instrument capability has improved the sensitivity, accuracy, coverage, and resolution of the observations. Through striking illustrations and clear explanations, the book summarizes many examples of analyses from the satellite nightlight data record—examples that themselves shine light on the ever-changing environment and our human impact on Earth.\\n\\nThis artful volume reaffirms our human ability to harness technology and science to observe and understand Earth for the benefit of all humankind. Above all, it once again illuminates the beauty and majesty of our home planet—at all hours.\\n\\n*Michael H. Freilich*\\n\\nFormer Director, Earth Science Division  \\n(October 23, 2010 - February 28, 2019)  \\nNASA Science Mission Directorate\\n\\n<!-- PageNumber=\\\"xiii\\\" -->\"},{\"ref_id\":1,\"content\":\"## Foreword\\n\\nNASA's Earth at Night explores the brilliance of our planet when it is in darkness. It is a compilation of stories depicting the interactions between science and wonder, and I am pleased to share this visually stunning and captivating exploration of our home planet.\\n\\nFrom space, our Earth looks tranquil. The blue ethereal vastness of the oceans harmoniously shares the space with verdant green land—an undercurrent of gentleness and solitude. But spending time gazing at the images presented in this book, our home planet at night instantly reveals a different reality. Beautiful, filled with glowing communities, natural wonders, and striking illumination, our world is bustling with activity and life.\\n\\nDarkness is not void of illumination. It is the contrast, the area between light and dark, that is often the most illustrative. Darkness reminds me of where I came from and where I am now—from a small town in the mountains, to the unique vantage point of the Nation's capital. Darkness is where dreamers and learners of all ages peer into the universe and think of questions about themselves and their space in the cosmos. Light is where they work, where they gather, and take time together.\\n\\nNASA's spacefaring satellites have compiled an unprecedented record of our Earth, and its luminescence in darkness, to captivate and spark curiosity. These missions see the contrast between dark and light through the lenses of scientific instruments. Our home planet is full of complex and dynamic cycles and processes. These soaring observers show us new ways to discern the nuances of light created by natural and human-made sources, such as auroras, wildfires, cities, phytoplankton, and volcanoes.\\n\\n---\\n\\n*Earth at Night*  \\nPage x ☒\"},{\"ref_id\":2,\"content\":\"January 2012 and is being processed on a daily basis within 3-5 hours after acquisition, enabling both near-real-time uses and long-term monitoring applications. Detailed information on product generation, applications, and documentation can be found at the product's website: [https://blackmarble.gsfc.nasa.gov](https://blackmarble.gsfc.nasa.gov).\\n\\nThese products are not just visually appealing maps of Earth at night. They provide a global, nighttime, environmental-science-quality dataset that supports all major scientific disciplines focused on the nocturnal environment. These data are particularly valuable for targeted studies of phenomena and processes including:\\n\\n- Light pollution\\n- Urbanization science\\n- Disaster response\\n- Clouds and aerosols\\n- Ocean studies at night\\n- Applications related to energy access, disaster risk reduction, and resilience\\n\\nThe daily records are essential for a broad range of users, such as city planners, ecologists, economists, and emergency responders—especially when assessing damage from major storms like Hurricane Maria (see pages 118-121 of this volume). Because nighttime light largely results from human activity, these products enhance our understanding of human processes on land and at sea, as well as interactions between human systems and the broader environment.\\n\\n---\\n\\n*Page Footer: 172 Earth at Night*\"},{\"ref_id\":3,\"content\":\"# Earth at Night: The Black Marble\\n\\nEnshrouded in nighttime darkness, Earth appears more as a black marble from space—one shimmering with light. The human search for light in darkness is long-standing. Many of our myths and religions focus on this search, yet it is only within the past century that humans have gained the ability to take flight—first to the skies and later into space—providing new vantage points from which to view Earth and the twinkling lights below, clearly visible at night.\\n\\nFor nearly 25 years, satellite images of Earth at night have served as a fundamental research tool, while also stoking public curiosity. These images paint an expansive and revealing picture, showing how humans have illuminated and shaped the planet in profound ways since the invention of the light bulb 140 years ago.\\n\\nThese lights and the darkness tell stories about our planet—stories that this volume will present for your consideration. *Earth at Night* will show how humans and natural phenomena light up the darkness, and how and why scientists have observed Earth's nightlights for more than four decades using both their own eyes and spaceborne instruments. It is an engaging and fascinating story; come along for the adventure!\\n\\n---\\n\\n## Figure: NASA's Black Marble\\n\\nThese images of Earth at night were created in 2012 over 9 days in April and 13 days in October. It took the Suomi National Polar-orbiting Partnership (NPP) satellite 312 orbits and 2.5 terabytes of data to get a clear shot of every parcel of Earth's land surface and islands. These new data were mapped over existing Blue Marble imagery to provide a realistic nighttime view of the planet.\\n\\n**Verbal Description of the Figure:**\\n\\nThe image is a composite of three globe views of Earth at night, each depicting different regions of the planet shrouded in night.\\n\\n1. **The Americas:**  \\n   North and South America are depicted. Bright clusters of lights concentrate along the eastern half of the United States and in metropolitan regions of South America, such as around São Paulo and Buenos Aires. The western part of the continents and interior areas are much darker, with few lights indicating large unpopulated or less-densely-populated areas.\\n\\n2. **Europe, Africa, and the Middle East:**  \\n   Europe is conspicuously bright, especially across Western and Central Europe with a dense network of city lights. The Nile River in Egypt is a visible line of illumination running through otherwise dark northern Africa. The Arabian Peninsula and parts of India also reveal significant concentrations of light, while most of Africa remains relatively dark except for scattered metropolitan areas.\\n\\n3. **Asia and Australia:**  \\n   Eastern Asia, particularly eastern China, the Korean peninsula, and Japan, show vast and dense networks of bright lights. Southeast Asia features concentrated lights in and around major cities. Australia is predominantly dark, with lights visible mainly around the edges in its populated coastal cities.\\n\\nOverall, the illuminated areas correspond to major urban and industrial centers, highlighting patterns of human habitation, economic development, and electricity usage across the globe. The large dark areas mark oceans, deserts, dense forests, and sparsely populated regions.\\n\\n> *For technical details on how these images were produced, see \\\"Making a Cloud-Free, Global, Earth-at-Night Image Using NASA's Black Marble Product Suite\\\" in Appendix B.*\\n\\n<!-- PageNumber=\\\"5\\\" -->\"},{\"ref_id\":4,\"content\":\"# Table of Contents\\n\\n## Effects of War\\n\\n| Topic                                                                             | Page |\\n|-----------------------------------------------------------------------------------|------|\\n| Conflict in the Middle East — Syria                                               | 140  |\\n| Nightlights Change in the Middle East — Syria and Iraq                            | 142  |\\n\\n## Mining\\n\\n| Topic                                                                             | Page |\\n|-----------------------------------------------------------------------------------|------|\\n| Mining                                                                           | 144  |\\n| Shale Revolution: As Clear as Night and Day — South Texas                        | 144  |\\n| Ten Percent of the World's Gas Flares in One Spot — Nigeria                      | 146  |\\n| Gas Flares in Bahía de Campeche — Gulf of Mexico                                 | 148  |\\n| Gas Drilling — North Dakota                                                      | 150  |\\n| Connection Between Gas Flaring and Arctic Pollution — North Dakota               | 152  |\\n\\n## Sea-Going Vessels\\n\\n| Topic                                                                             | Page |\\n|-----------------------------------------------------------------------------------|------|\\n| Sea-Going Vessels                                                               | 154  |\\n| Something Fishy in the Atlantic Night — South Atlantic Ocean                     | 154  |\\n| Korea and the Yellow Sea — Korean Peninsula                                      | 156  |\\n\\n## Holiday Lights\\n\\n| Topic                                                                             | Page |\\n|-----------------------------------------------------------------------------------|------|\\n| Holiday Lights                                                                  | 158  |\\n| Bursting with Holiday Energy — United States                                      | 158  |\\n| The Lights of Ramadan and Eid al-Fitr — Middle East                              | 164  |\\n\\n## Epilogue and Appendices\\n\\n| Topic                                                                             | Page |\\n|-----------------------------------------------------------------------------------|------|\\n| Epilogue                                                                        | 168  |\\n| Appendix A: NASA's Black Marble Product Suite                                    | 170  |\\n| Appendix B: Making a Cloud-Free, Global, Earth-at-Night Image Using NASA's Black Marble Product Suite | 173  |\\n| Appendix C: OLS and VIIRS Technical Details                                      | 176  |\\n| Appendix D: Additional Credits and Information                                   | 179  |\\n\\n\\n---\\n\\n*Page viii, Earth at Night*\"}]\n"
     ]
    }
   ],
   "source": [
    "# Test the knowledge agent configuration and retrieval functionality\n",
    "# This cell performs a direct test of the retrieval system before full agent integration\n",
    "\n",
    "# Get detailed information about our knowledge agent\n",
    "agent_info = index_client.get_agent(agent_name)\n",
    "# Display which indexes the agent can search\n",
    "# This should show our \"earth-at-night\" index\n",
    "print(\"Agent target indexes:\", [ti.index_name for ti in agent_info.target_indexes])\n",
    "\n",
    "# Create a test message to verify the retrieval system works\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the main topic of the NASA Earth at Night e-book?\"}\n",
    "]\n",
    "\n",
    "# Perform a direct retrieval test using the knowledge agent client\n",
    "# This bypasses the AI agent and tests the search functionality directly\n",
    "retrieval_result = agent_client.retrieve(\n",
    "    retrieval_request=KnowledgeAgentRetrievalRequest(\n",
    "        # Convert test message to the format expected by the knowledge agent\n",
    "        messages=[\n",
    "            KnowledgeAgentMessage(\n",
    "                role=msg[\"role\"], \n",
    "                content=[KnowledgeAgentMessageTextContent(text=msg[\"content\"])]\n",
    "            ) \n",
    "            for msg in test_messages\n",
    "        ],\n",
    "        \n",
    "        # Configure search parameters for the test\n",
    "        target_index_params=[\n",
    "            KnowledgeAgentIndexParams(\n",
    "                index_name=index_name,      # Search our Earth at night index\n",
    "                reranker_threshold=2.5      # Use same threshold as in production\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the raw retrieval response\n",
    "# This shows what the knowledge agent found and how it formatted the results\n",
    "# The response should contain relevant information about the NASA e-book\n",
    "print(\"Retrieval response:\", retrieval_result.response[0].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b293e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== agentic_retrieval Function Storage Information ===\n",
      "Function name: agentic_retrieval\n",
      "Function type: <class 'function'>\n",
      "Function location in memory: 1446108892032\n",
      "\n",
      "FunctionTool contains: <azure.ai.agents.models._patch.FunctionTool object at 0x00000150AE1EA8D0>\n",
      "FunctionTool type: <class 'azure.ai.agents.models._patch.FunctionTool'>\n",
      "\n",
      "Toolset object: <azure.ai.agents.models._patch.ToolSet object at 0x00000150AE08A2C0>\n",
      "Toolset type: <class 'azure.ai.agents.models._patch.ToolSet'>\n",
      "\n",
      "Function signature: \n",
      "Agentic retrieval function that searches NASA e-book data about Earth at night.\n",
      "\n",
      "This function serves as a \"tool\" that the AI agent can call when it needs to\n",
      "retrieve information from the search index. The agent decides autonomously\n",
      "when to use this tool based on the user's query.\n",
      "\n",
      "Returns:\n",
      "    str: JSON-formatted search results with reference IDs for citation\n",
      "\n",
      "The function performs these steps:\n",
      "1. Gets recent conversation history for context\n",
      "2. Sends the conversation to the knowledge agent for intelligent retrieval\n",
      "3. Stores detailed retrieval results for later analysis\n",
      "4. Returns the formatted response to the AI agent\n",
      "\n",
      "\n",
      "=== What this function is NOT stored in: ===\n",
      "❌ Not in a database\n",
      "❌ Not in a file on disk\n",
      "❌ Not persisted in Azure\n",
      "❌ Not saved between notebook sessions\n",
      "✅ Only exists in current Python runtime memory\n",
      "✅ Must be redefined if kernel restarts\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate where the agentic_retrieval function is stored\n",
    "print(\"=== agentic_retrieval Function Storage Information ===\")\n",
    "\n",
    "# 1. The function exists in the current Python namespace (memory)\n",
    "print(f\"Function name: {agentic_retrieval.__name__}\")\n",
    "print(f\"Function type: {type(agentic_retrieval)}\")\n",
    "print(f\"Function location in memory: {id(agentic_retrieval)}\")\n",
    "\n",
    "# 2. Check if it's part of the FunctionTool\n",
    "print(f\"\\nFunctionTool contains: {functions}\")\n",
    "print(f\"FunctionTool type: {type(functions)}\")\n",
    "\n",
    "# 3. Check if it's registered with the agent's toolset\n",
    "print(f\"\\nToolset object: {toolset}\")\n",
    "print(\"Toolset type:\", type(toolset))\n",
    "\n",
    "# 4. The function's docstring and signature\n",
    "print(f\"\\nFunction signature: {agentic_retrieval.__doc__}\")\n",
    "\n",
    "# 5. Show where it's NOT stored (it's ephemeral)\n",
    "print(\"\\n=== What this function is NOT stored in: ===\")\n",
    "print(\"❌ Not in a database\")\n",
    "print(\"❌ Not in a file on disk\") \n",
    "print(\"❌ Not persisted in Azure\")\n",
    "print(\"❌ Not saved between notebook sessions\")\n",
    "print(\"✅ Only exists in current Python runtime memory\")\n",
    "print(\"✅ Must be redefined if kernel restarts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5b621",
   "metadata": {},
   "source": [
    "## Start a chat with the agent\n",
    "\n",
    "During the chat, you use the standard Azure AI agent tool calling APIs.  We send the message with questions, and the agent decides when to retrieve knowledge from your search index using agentic retrieval.\n",
    "\n",
    "The remaining cells take a closer look at output and show how to add another turn to the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1fc04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: The suburban belts display larger December brightening than urban cores likely because these areas have more lighting installed that responds to seasonal and decorative changes, such as holiday lights, whereas absolute light levels remain higher in the urban cores due to constant and dense urban lighting infrastructure\n",
      "\n",
      "\n",
      "Regarding Phoenix, the nighttime street grid is sharply visible from space because the metropolitan area is laid out along a regular and extensive grid of city blocks and streets\n",
      " This grid includes lighting from major transportation corridors, industrial and commercial facilities, and residential streets\n",
      " The lighting at intersections and along streets creates a distinct grid pattern visible from low-earth orbit vantage points like the ISS\n",
      " In contrast, large stretches of interstate between Midwestern cities remain comparatively dim likely because they have less urban development or street lighting coverage along those routes, and the lighting is less regular or less dense\n",
      " Phoenix's urban lighting pattern is enhanced by the widespread use of streetlights and commercial lighting, and the city's layout encourages outward growth along the grid, creating a vivid illuminated pattern at night [ref_id 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import classes for controlling agent tool selection\n",
    "from azure.ai.agents.models import AgentsNamedToolChoice, AgentsNamedToolChoiceType, FunctionName\n",
    "\n",
    "# Create a user message with complex questions about Earth at night phenomena\n",
    "# These questions test the agent's ability to:\n",
    "# - Handle multi-part queries\n",
    "# - Understand scientific concepts\n",
    "# - Find specific information in the knowledge base\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"\"\"\n",
    "        Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\n",
    "        Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Execute the agent run with forced tool usage\n",
    "# We explicitly force the agent to use our agentic_retrieval tool to ensure\n",
    "# it searches for information rather than relying only on its training data\n",
    "run = project_client.agents.runs.create_and_process(\n",
    "    thread_id=thread.id,        # Use our conversation thread\n",
    "    agent_id=agent.id,          # Use our created agent\n",
    "    \n",
    "    # Force the agent to use our agentic_retrieval function\n",
    "    # This ensures the agent will search the index for information\n",
    "    tool_choice=AgentsNamedToolChoice(\n",
    "        type=AgentsNamedToolChoiceType.FUNCTION, \n",
    "        function=FunctionName(name=\"agentic_retrieval\")\n",
    "    ),\n",
    "    \n",
    "    toolset=toolset  # Provide access to our retrieval tool\n",
    ")\n",
    "\n",
    "# Check if the agent run completed successfully\n",
    "# If it failed, we want to know why (could be model issues, tool problems, etc.)\n",
    "if run.status == \"failed\":\n",
    "    raise RuntimeError(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Get the agent's response to our question\n",
    "# The agent should have used our retrieval tool to find relevant information\n",
    "# and formatted it according to our instructions (with citation references)\n",
    "output = project_client.agents.messages.get_last_message_text_by_role(\n",
    "    thread_id=thread.id, \n",
    "    role=\"assistant\"\n",
    ").text.value\n",
    "\n",
    "# Display the agent's response with improved formatting\n",
    "# Replace periods with newlines for better readability of the detailed response\n",
    "print(\"Agent response:\", output.replace(\".\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88be35",
   "metadata": {},
   "source": [
    "## Review retrieval activity and results\n",
    "\n",
    "Each retrieval response from Azure AI Search includes the unified string (grounding data from search search results), the query plan, and  reference data showing which chunks of source document contributed content to the unified string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b90fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval activity\n",
      "[\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"ModelQueryPlanning\",\n",
      "    \"input_tokens\": 1383,\n",
      "    \"output_tokens\": 560\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"AzureSearchQuery\",\n",
      "    \"target_index\": \"12-earth-at-night\",\n",
      "    \"query\": {\n",
      "      \"search\": \"Why do suburban areas show greater December brightening in light levels compared to urban cores despite higher absolute light levels downtown?\"\n",
      "    },\n",
      "    \"query_time\": \"2025-07-20T12:49:23.361Z\",\n",
      "    \"count\": 0,\n",
      "    \"elapsed_ms\": 244\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"AzureSearchQuery\",\n",
      "    \"target_index\": \"12-earth-at-night\",\n",
      "    \"query\": {\n",
      "      \"search\": \"Why is the Phoenix nighttime street grid so distinctly visible from space, while large stretches of interstate highways between Midwestern cities appear comparatively dim?\"\n",
      "    },\n",
      "    \"query_time\": \"2025-07-20T12:49:23.570Z\",\n",
      "    \"count\": 2,\n",
      "    \"elapsed_ms\": 208\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"type\": \"AzureSearchSemanticRanker\",\n",
      "    \"input_tokens\": 49699\n",
      "  }\n",
      "]\n",
      "Retrieval results\n",
      "[\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 2,\n",
      "    \"doc_key\": \"earth_at_night_508_page_104_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"1\",\n",
      "    \"activity_source\": 2,\n",
      "    \"doc_key\": \"earth_at_night_508_page_105_verbalized\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Import JSON for pretty-printing the retrieval analysis\n",
    "import json\n",
    "\n",
    "# Retrieve the detailed retrieval results for the message we just processed\n",
    "# This gives us insight into what the knowledge agent actually did behind the scenes\n",
    "retrieval_result = retrieval_results.get(message.id)\n",
    "\n",
    "# Ensure we have retrieval results for this message\n",
    "# This could fail if the agent didn't use our retrieval tool or if there was an error\n",
    "if retrieval_result is None:\n",
    "    raise RuntimeError(f\"No retrieval results found for message {message.id}\")\n",
    "\n",
    "# Display the retrieval activity log\n",
    "# This shows the step-by-step process the knowledge agent went through:\n",
    "# - Query analysis and understanding\n",
    "# - Search strategy selection (keyword, vector, semantic, hybrid)\n",
    "# - Query reformulation and expansion\n",
    "# - Result ranking and filtering\n",
    "print(\"Retrieval activity\")\n",
    "print(json.dumps([activity.as_dict() for activity in retrieval_result.activity], indent=2))\n",
    "\n",
    "# Display the retrieval results and references\n",
    "# This shows:\n",
    "# - Which documents were found and considered relevant\n",
    "# - Relevance scores for each result\n",
    "# - The specific chunks of text that contributed to the response\n",
    "# - Reference IDs that should appear in the agent's response citations\n",
    "print(\"Retrieval results\")\n",
    "print(json.dumps([reference.as_dict() for reference in retrieval_result.references], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bfb9f",
   "metadata": {},
   "source": [
    "## Continue the conversation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9478191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: To find lava at night, you can look for the natural glow emitted by hot lava flows at active volcanoes\n",
      " This glow is visible from space and distinct from city lights because the lava emits bright red and orange lights\n",
      " Such glowing lava flows can be tracked and monitored using nighttime satellite imagery that detects the thermal and visible light emitted by the lava\n",
      " For example, eruptions like those of Mount Etna in Italy and Kilauea in Hawaii show bright spots of lava flow that stand out in nighttime images\n",
      " These observations can be enhanced by moonlight, which helps illuminate the surrounding area and allows for better visibility of the glowing lava [ref_id 1, 2, 3, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation with a new question\n",
    "# This demonstrates multi-turn conversation capabilities and context maintenance\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,  # Use the same thread to maintain conversation context\n",
    "    role=\"user\",\n",
    "    content=\"How do I find lava at night? Use the retrieval tool to answer this question.\"\n",
    ")\n",
    "# Note: This question explicitly asks the agent to use the retrieval tool\n",
    "# This tests whether the agent can find information about thermal detection\n",
    "# and volcanic activity observation from space\n",
    "\n",
    "# Execute another agent run with the same configuration\n",
    "# The agent now has the context of the previous conversation\n",
    "run = project_client.agents.runs.create_and_process(\n",
    "    thread_id=thread.id,        # Same conversation thread for context\n",
    "    agent_id=agent.id,          # Same agent instance\n",
    "    \n",
    "    # Again force the use of our retrieval tool\n",
    "    # This ensures consistent behavior for demonstration purposes\n",
    "    tool_choice=AgentsNamedToolChoice(\n",
    "        type=AgentsNamedToolChoiceType.FUNCTION, \n",
    "        function=FunctionName(name=\"agentic_retrieval\")\n",
    "    ),\n",
    "    \n",
    "    toolset=toolset  # Same toolset with our retrieval function\n",
    ")\n",
    "\n",
    "# Check for execution errors\n",
    "if run.status == \"failed\":\n",
    "    raise RuntimeError(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Get the agent's response to the lava detection question\n",
    "# The response should incorporate both the new query and any relevant context\n",
    "# from the previous conversation if applicable\n",
    "output = project_client.agents.messages.get_last_message_text_by_role(\n",
    "    thread_id=thread.id, \n",
    "    role=\"assistant\"\n",
    ").text.value\n",
    "\n",
    "# Display the response with formatting for better readability\n",
    "print(\"Agent response:\", output.replace(\".\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366ed37",
   "metadata": {},
   "source": [
    "## Review retrieval activity and results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c063c45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval activity\n",
      "[\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"ModelQueryPlanning\",\n",
      "    \"input_tokens\": 1622,\n",
      "    \"output_tokens\": 129\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"AzureSearchQuery\",\n",
      "    \"target_index\": \"12-earth-at-night\",\n",
      "    \"query\": {\n",
      "      \"search\": \"How do I find lava at night?\"\n",
      "    },\n",
      "    \"query_time\": \"2025-07-20T12:50:54.311Z\",\n",
      "    \"count\": 6,\n",
      "    \"elapsed_ms\": 1721\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"AzureSearchSemanticRanker\",\n",
      "    \"input_tokens\": 23060\n",
      "  }\n",
      "]\n",
      "Retrieval results\n",
      "[\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_44_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"1\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_65_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"2\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_64_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"3\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_66_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"4\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_60_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"5\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_46_verbalized\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Analyze the retrieval results for the second question (about finding lava at night)\n",
    "# This allows us to compare how the knowledge agent handled different types of queries\n",
    "\n",
    "# Get the retrieval results for the lava detection question\n",
    "retrieval_result = retrieval_results.get(message.id)\n",
    "\n",
    "# Verify we have results for this message\n",
    "if retrieval_result is None:\n",
    "    raise RuntimeError(f\"No retrieval results found for message {message.id}\")\n",
    "\n",
    "# Display the retrieval activity for the lava detection query\n",
    "# Compare this with the previous query to see how the agent:\n",
    "# - Adapted its search strategy for a different topic\n",
    "# - Used different keywords or semantic understanding\n",
    "# - Potentially found different types of content\n",
    "print(\"Retrieval activity\")\n",
    "print(json.dumps([activity.as_dict() for activity in retrieval_result.activity], indent=2))\n",
    "\n",
    "# Display the specific results and references found for lava detection\n",
    "# This might include information about:\n",
    "# - Thermal imaging techniques\n",
    "# - Infrared detection methods\n",
    "# - Volcanic monitoring from satellite imagery\n",
    "# - Temperature signatures of geological activity\n",
    "print(\"Retrieval results\")\n",
    "print(json.dumps([reference.as_dict() for reference in retrieval_result.references], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04661708",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0926264d",
   "metadata": {},
   "source": [
    "## Clean up objects and resources\n",
    "\n",
    "If you no longer need the resources, be sure to delete them from your Azure subscription.  You can also delete individual objects to start over.\n",
    "\n",
    "### Delete the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "index_client.delete_agent(agent_name)\n",
    "print(f\"Knowledge agent '{agent_name}' deleted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2e274",
   "metadata": {},
   "source": [
    "### Delete the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "index_client.delete_index(index)\n",
    "print(f\"Index '{index_name}' deleted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1baead",
   "metadata": {},
   "source": [
    "### Delete the AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the AI agent from Azure AI Foundry to clean up resources\n",
    "\n",
    "# Use the project_client to delete the agent by its ID\n",
    "project_client.agents.delete_agent(agent.id)\n",
    "print(f\"AI agent '{agent.name}' deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
